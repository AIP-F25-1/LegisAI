{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning: /Users/nithish/Desktop/USA/CFR-regulations\n",
      "\n",
      "===== BASIC SUMMARY =====\n",
      "Files: 245\n",
      "Total size: 1.09 GB\n",
      "\n",
      "Top extensions (by count):\n",
      "ext\n",
      ".xml    244\n",
      "          1\n",
      "\n",
      "Top subfolders (top_level) by file count:\n",
      "top_level\n",
      "title-40    37\n",
      "title-26    22\n",
      "title-7     15\n",
      "title-50    13\n",
      "title-12    10\n",
      "title-21     9\n",
      "title-29     9\n",
      "title-49     9\n",
      "title-46     9\n",
      "title-48     7\n",
      "title-32     6\n",
      "title-42     5\n",
      "title-45     5\n",
      "title-24     5\n",
      "title-47     5\n",
      "title-17     5\n",
      "title-14     5\n",
      "title-20     4\n",
      "title-41     4\n",
      "title-10     4\n",
      "\n",
      "Largest 20 files:\n",
      "                            relpath size_human               mtime  ext\n",
      "title-40/CFR-2023-title40-vol20.xml   12.00 MB 2025-07-09 19:15:52 .xml\n",
      " title-15/CFR-2023-title15-vol2.xml   10.68 MB 2025-07-09 19:15:40 .xml\n",
      " title-40/CFR-2023-title40-vol3.xml   10.02 MB 2025-07-09 19:15:52 .xml\n",
      "title-40/CFR-2023-title40-vol32.xml    9.93 MB 2025-07-09 19:15:52 .xml\n",
      "title-40/CFR-2023-title40-vol33.xml    9.66 MB 2025-07-09 19:15:54 .xml\n",
      " title-49/CFR-2023-title49-vol2.xml    9.55 MB 2025-07-09 19:15:58 .xml\n",
      " title-10/CFR-2023-title10-vol3.xml    9.50 MB 2025-07-09 19:15:40 .xml\n",
      "title-40/CFR-2023-title40-vol26.xml    9.30 MB 2025-07-09 19:15:52 .xml\n",
      "title-26/CFR-2023-title26-vol10.xml    8.68 MB 2025-07-09 19:15:46 .xml\n",
      " title-20/CFR-2023-title20-vol2.xml    8.32 MB 2025-07-09 19:15:42 .xml\n",
      "title-40/CFR-2023-title40-vol14.xml    8.26 MB 2025-07-09 19:15:52 .xml\n",
      "title-40/CFR-2023-title40-vol23.xml    8.20 MB 2025-07-09 19:15:52 .xml\n",
      "title-40/CFR-2023-title40-vol25.xml    7.87 MB 2025-07-09 19:15:50 .xml\n",
      "  title-7/CFR-2023-title7-vol10.xml    7.70 MB 2025-07-09 19:15:36 .xml\n",
      "title-40/CFR-2023-title40-vol22.xml    7.68 MB 2025-07-09 19:15:50 .xml\n",
      " title-47/CFR-2023-title47-vol5.xml    7.64 MB 2025-07-09 19:15:56 .xml\n",
      " title-18/CFR-2023-title18-vol1.xml    7.62 MB 2025-07-09 19:15:40 .xml\n",
      " title-33/CFR-2023-title33-vol2.xml    7.60 MB 2025-07-09 19:15:56 .xml\n",
      " title-26/CFR-2023-title26-vol4.xml    7.57 MB 2025-07-09 19:15:44 .xml\n",
      "title-26/CFR-2023-title26-vol13.xml    7.45 MB 2025-07-09 19:15:44 .xml\n",
      "\n",
      "üìÑ Inventory saved: /Users/nithish/Desktop/USA/CFR-regulations/_inspections/file_inventory.csv\n",
      "\n",
      "===== XML SAMPLE PARSE (up to 5) =====\n",
      "‚Ä¢ title-40/CFR-2023-title40-vol20.xml  (12.00 MB)\n",
      "  - tag counts: TTITLE:865, SECTION:344, SUBPART:10, BTITLE:1, CFRTITLE:1, APPENDIX:1, PART:1, CHAPTER:1\n",
      "‚Ä¢ title-15/CFR-2023-title15-vol2.xml  (10.68 MB)\n",
      "  - tag counts: SECTION:537, APPENDIX:102, PART:64, SUBPART:42, TTITLE:8, CHAPTER:3, BTITLE:1, CFRTITLE:1\n",
      "‚Ä¢ title-40/CFR-2023-title40-vol3.xml  (10.02 MB)\n",
      "  - tag counts: SECTION:460, TTITLE:145, SUBPART:40, BTITLE:1, CFRTITLE:1, PART:1, CHAPTER:1, TITLE:1\n",
      "‚Ä¢ title-40/CFR-2023-title40-vol32.xml  (9.93 MB)\n",
      "  - tag counts: SECTION:1613, TTITLE:1363, SUBPART:548, PART:46, APPENDIX:38, BTITLE:1, CFRTITLE:1, CHAPTER:1\n",
      "‚Ä¢ title-40/CFR-2023-title40-vol33.xml  (9.66 MB)\n",
      "  - tag counts: SECTION:2532, SUBPART:64, TTITLE:31, PART:14, BTITLE:1, CFRTITLE:1, CHAPTER:1, TITLE:1\n",
      "\n",
      "(No JSON files detected to sample.)\n",
      "\n",
      "===== SHA256 of 5 Largest Files =====\n",
      "‚Ä¢ title-40/CFR-2023-title40-vol20.xml  12.00 MB  sha256=eab7cd60f148c9ef‚Ä¶\n",
      "‚Ä¢ title-15/CFR-2023-title15-vol2.xml  10.68 MB  sha256=d835446ee86602eb‚Ä¶\n",
      "‚Ä¢ title-40/CFR-2023-title40-vol3.xml  10.02 MB  sha256=cbf5ce6646a4ba08‚Ä¶\n",
      "‚Ä¢ title-40/CFR-2023-title40-vol32.xml  9.93 MB  sha256=90d0dbe9294cb8a6‚Ä¶\n",
      "‚Ä¢ title-40/CFR-2023-title40-vol33.xml  9.66 MB  sha256=5d3b74730bf141bd‚Ä¶\n",
      "\n",
      "üìÑ Extra summaries saved: /Users/nithish/Desktop/USA/CFR-regulations/_inspections/by_extension.csv and by_top_level.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "CFR Data Inspector\n",
    "Scans a directory, summarizes files, and lightly inspects XML/JSON samples.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import hashlib\n",
    "import mimetypes\n",
    "import datetime as dt\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "except ImportError:\n",
    "    print(\"Installing pandas ... (Ctrl+C to cancel)\")\n",
    "    os.system(f\"{sys.executable} -m pip install -q pandas\")\n",
    "    import pandas as pd\n",
    "\n",
    "try:\n",
    "    import lxml.etree as ET  # better/faster than xml.etree for big files\n",
    "except ImportError:\n",
    "    print(\"Installing lxml ... (Ctrl+C to cancel)\")\n",
    "    os.system(f\"{sys.executable} -m pip install -q lxml\")\n",
    "    import lxml.etree as ET\n",
    "\n",
    "# ---------- configure here ----------\n",
    "BASE_DIR = \"/Users/nithish/Desktop/USA/CFR-regulations\"\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"_inspections\")\n",
    "SAMPLE_XML_TO_PARSE = 5      # how many XML files to sample\n",
    "SAMPLE_JSON_TO_PARSE = 5     # how many JSON files to sample\n",
    "HASH_LARGEST_N = 5           # hash top N largest files (helps detect duplicates)\n",
    "# ------------------------------------\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "def human(nbytes: int) -> str:\n",
    "    units = [\"B\",\"KB\",\"MB\",\"GB\",\"TB\"]\n",
    "    i = 0\n",
    "    x = float(nbytes)\n",
    "    while x >= 1024 and i < len(units)-1:\n",
    "        x /= 1024.0\n",
    "        i += 1\n",
    "    return f\"{x:.2f} {units[i]}\"\n",
    "\n",
    "def file_sha256(path, bufsize=1024*1024):\n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(bufsize)\n",
    "            if not b:\n",
    "                break\n",
    "            h.update(b)\n",
    "    return h.hexdigest()\n",
    "\n",
    "def walk_dir(base_dir: str):\n",
    "    rows = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for name in files:\n",
    "            fpath = os.path.join(root, name)\n",
    "            try:\n",
    "                st = os.stat(fpath)\n",
    "                size = st.st_size\n",
    "                mtime = dt.datetime.fromtimestamp(st.st_mtime)\n",
    "                ext = os.path.splitext(name)[1].lower()\n",
    "                mimetype, _ = mimetypes.guess_type(fpath)\n",
    "                rel = os.path.relpath(fpath, base_dir)\n",
    "                top_level = rel.split(os.sep)[0] if os.sep in rel else \".\"\n",
    "                rows.append({\n",
    "                    \"relpath\": rel,\n",
    "                    \"dir\": os.path.dirname(rel),\n",
    "                    \"top_level\": top_level,\n",
    "                    \"filename\": name,\n",
    "                    \"ext\": ext or \"\",\n",
    "                    \"mimetype\": mimetype or \"\",\n",
    "                    \"size_bytes\": size,\n",
    "                    \"size_human\": human(size),\n",
    "                    \"mtime\": mtime,\n",
    "                    \"abs_path\": fpath\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è  Could not stat {fpath}: {e}\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summarize_df(df: pd.DataFrame):\n",
    "    print(\"\\n===== BASIC SUMMARY =====\")\n",
    "    total = len(df)\n",
    "    total_bytes = int(df[\"size_bytes\"].sum()) if total else 0\n",
    "    print(f\"Files: {total:,}\")\n",
    "    print(f\"Total size: {human(total_bytes)}\")\n",
    "\n",
    "    if total == 0:\n",
    "        return\n",
    "\n",
    "    print(\"\\nTop extensions (by count):\")\n",
    "    by_ext = (df.groupby(\"ext\")[\"relpath\"].count()\n",
    "                .sort_values(ascending=False)\n",
    "                .head(20))\n",
    "    print(by_ext.to_string())\n",
    "\n",
    "    print(\"\\nTop subfolders (top_level) by file count:\")\n",
    "    by_top = (df.groupby(\"top_level\")[\"relpath\"].count()\n",
    "                .sort_values(ascending=False)\n",
    "                .head(20))\n",
    "    print(by_top.to_string())\n",
    "\n",
    "    print(\"\\nLargest 20 files:\")\n",
    "    largest = df.sort_values(\"size_bytes\", ascending=False).head(20)[\n",
    "        [\"relpath\",\"size_human\",\"mtime\",\"ext\"]\n",
    "    ]\n",
    "    print(largest.to_string(index=False))\n",
    "\n",
    "    # Save CSV for deeper analysis\n",
    "    csv_path = os.path.join(OUTPUT_DIR, \"file_inventory.csv\")\n",
    "    df.sort_values([\"ext\",\"size_bytes\"], ascending=[True, False]).to_csv(csv_path, index=False)\n",
    "    print(f\"\\nüìÑ Inventory saved: {csv_path}\")\n",
    "\n",
    "def sample_xml(df: pd.DataFrame, max_n=5):\n",
    "    xml_df = df[df[\"ext\"].isin([\".xml\",\".XML\"])].copy()\n",
    "    if xml_df.empty:\n",
    "        print(\"\\n(No XML files detected to sample.)\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n===== XML SAMPLE PARSE (up to {max_n}) =====\")\n",
    "    for _, row in xml_df.sort_values(\"size_bytes\", ascending=False).head(max_n).iterrows():\n",
    "        f = row[\"abs_path\"]\n",
    "        rel = row[\"relpath\"]\n",
    "        try:\n",
    "            # parse without huge memory blow-up\n",
    "            # using iterparse to count tags of interest quickly\n",
    "            tag_counts = Counter()\n",
    "            title_text = None\n",
    "\n",
    "            # Count common CFR-ish structural tags heuristically\n",
    "            interesting = re.compile(r\"(TITLE|CHAPTER|SUBCHAPTER|PART|SUBPART|SECTION|SECT|APPENDIX)$\", re.I)\n",
    "\n",
    "            for _, elem in ET.iterparse(f, events=(\"end\",), recover=True):\n",
    "                tag = elem.tag.split(\"}\")[-1]  # strip namespace if present\n",
    "                if interesting.search(tag):\n",
    "                    tag_counts[tag.upper()] += 1\n",
    "                # try capture a likely title/heading if present\n",
    "                if title_text is None and tag.upper() in {\"TITLE\",\"TITLEHD\",\"DOCTITLE\",\"FRDOCTITLE\"}:\n",
    "                    title_text = (elem.text or \"\").strip()[:200]\n",
    "                elem.clear()\n",
    "\n",
    "            print(f\"‚Ä¢ {rel}  ({row['size_human']})\")\n",
    "            if title_text:\n",
    "                print(f\"  - title/heading: {title_text}\")\n",
    "            if tag_counts:\n",
    "                top_tags = \", \".join(f\"{k}:{v}\" for k,v in tag_counts.most_common(8))\n",
    "                print(f\"  - tag counts: {top_tags}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è XML parse error for {rel}: {e}\")\n",
    "\n",
    "def sample_json(df: pd.DataFrame, max_n=5):\n",
    "    json_df = df[df[\"ext\"].isin([\".json\",\".JSON\"])].copy()\n",
    "    if json_df.empty:\n",
    "        print(\"\\n(No JSON files detected to sample.)\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n===== JSON SAMPLE PARSE (up to {max_n}) =====\")\n",
    "    for _, row in json_df.sort_values(\"size_bytes\", ascending=False).head(max_n).iterrows():\n",
    "        f = row[\"abs_path\"]\n",
    "        rel = row[\"relpath\"]\n",
    "        try:\n",
    "            with open(f, \"r\", encoding=\"utf-8\") as fh:\n",
    "                data = json.load(fh)\n",
    "            if isinstance(data, dict):\n",
    "                keys = list(data.keys())[:20]\n",
    "                print(f\"‚Ä¢ {rel}  ({row['size_human']})  keys: {keys}\")\n",
    "            elif isinstance(data, list):\n",
    "                print(f\"‚Ä¢ {rel}  ({row['size_human']})  list len: {len(data)}\")\n",
    "            else:\n",
    "                print(f\"‚Ä¢ {rel}  ({row['size_human']})  type: {type(data)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è JSON parse error for {rel}: {e}\")\n",
    "\n",
    "def hash_largest(df: pd.DataFrame, n=5):\n",
    "    if df.empty:\n",
    "        return\n",
    "    print(f\"\\n===== SHA256 of {n} Largest Files =====\")\n",
    "    for _, row in df.sort_values(\"size_bytes\", ascending=False).head(n).iterrows():\n",
    "        f = row[\"abs_path\"]\n",
    "        try:\n",
    "            h = file_sha256(f)\n",
    "            print(f\"‚Ä¢ {row['relpath']}  {row['size_human']}  sha256={h[:16]}‚Ä¶\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è Could not hash {row['relpath']}: {e}\")\n",
    "\n",
    "def main():\n",
    "    if not os.path.isdir(BASE_DIR):\n",
    "        print(f\"‚ùå Directory not found: {BASE_DIR}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(f\"Scanning: {BASE_DIR}\")\n",
    "    df = walk_dir(BASE_DIR)\n",
    "    summarize_df(df)\n",
    "    sample_xml(df, max_n=SAMPLE_XML_TO_PARSE)\n",
    "    sample_json(df, max_n=SAMPLE_JSON_TO_PARSE)\n",
    "    hash_largest(df, n=HASH_LARGEST_N)\n",
    "\n",
    "    # Simple pivot tables saved for quick glance in Excel/Numbers\n",
    "    if not df.empty:\n",
    "        by_ext = (df.groupby(\"ext\")[\"relpath\"].count()\n",
    "                    .sort_values(ascending=False).rename(\"count\")).reset_index()\n",
    "        by_ext.to_csv(os.path.join(OUTPUT_DIR, \"by_extension.csv\"), index=False)\n",
    "\n",
    "        by_top = (df.groupby(\"top_level\")[\"relpath\"].count()\n",
    "                    .sort_values(ascending=False).rename(\"count\")).reset_index()\n",
    "        by_top.to_csv(os.path.join(OUTPUT_DIR, \"by_top_level.csv\"), index=False)\n",
    "\n",
    "        print(f\"\\nüìÑ Extra summaries saved: {os.path.join(OUTPUT_DIR,'by_extension.csv')} and by_top_level.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 229637 sections ‚Üí /Users/nithish/Desktop/USA/CFR-regulations/_processed/cfr_sections.csv\n",
      "‚úÖ Also saved to SQLite ‚Üí /Users/nithish/Desktop/USA/CFR-regulations/_processed/cfr_sections.db\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import lxml.etree as ET\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "BASE_DIR = \"/Users/nithish/Desktop/USA/CFR-regulations\"\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"_processed\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "records = []\n",
    "\n",
    "def extract_sections(xml_file):\n",
    "    try:\n",
    "        context = ET.iterparse(xml_file, events=(\"end\",), recover=True)\n",
    "        current_title = os.path.basename(xml_file).split(\"-\")[2]  # e.g. CFR-2023-title40-vol1.xml -> title40\n",
    "        for _, elem in context:\n",
    "            tag = elem.tag.split(\"}\")[-1].upper()\n",
    "            if tag == \"SECTION\":\n",
    "                sectno = elem.findtext(\".//SECTNO\")\n",
    "                subject = elem.findtext(\".//SUBJECT\")\n",
    "                paras = [p.text for p in elem.findall(\".//P\") if p.text]\n",
    "                text = \"\\n\".join(paras).strip()\n",
    "                records.append({\n",
    "                    \"title\": current_title,\n",
    "                    \"file\": os.path.basename(xml_file),\n",
    "                    \"section_number\": (sectno or \"\").strip(),\n",
    "                    \"heading\": (subject or \"\").strip(),\n",
    "                    \"text\": text\n",
    "                })\n",
    "            elem.clear()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error parsing {xml_file}: {e}\")\n",
    "\n",
    "# Walk all XMLs\n",
    "for root, _, files in os.walk(BASE_DIR):\n",
    "    for f in files:\n",
    "        if f.lower().endswith(\".xml\"):\n",
    "            fpath = os.path.join(root, f)\n",
    "            extract_sections(fpath)\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(records)\n",
    "csv_path = os.path.join(OUTPUT_DIR, \"cfr_sections.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "print(f\"‚úÖ Extracted {len(df)} sections ‚Üí {csv_path}\")\n",
    "\n",
    "# Save to SQLite\n",
    "db_path = os.path.join(OUTPUT_DIR, \"cfr_sections.db\")\n",
    "conn = sqlite3.connect(db_path)\n",
    "df.to_sql(\"sections\", conn, if_exists=\"replace\", index=False)\n",
    "conn.close()\n",
    "print(f\"‚úÖ Also saved to SQLite ‚Üí {db_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import posixpath\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "BASE = \"https://static.case.law/\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"MyCaselawScraper/0.4 (+https://yourdomain.example.com)\"\n",
    "}\n",
    "\n",
    "# üîß toggle this: True = only download JSON files, False = download everything\n",
    "ONLY_JSON = True\n",
    "\n",
    "def list_dir(url):\n",
    "    \"\"\"Return list of hrefs in that directory (excluding parent link).\"\"\"\n",
    "    resp = requests.get(url, headers=HEADERS)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    links = []\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        href = a.get(\"href\")\n",
    "        if not href or href == \"../\":\n",
    "            continue\n",
    "        # Normalize absolute hrefs\n",
    "        if href.startswith(BASE):\n",
    "            href = href[len(BASE):]\n",
    "        href = href.lstrip(\"/\")   # remove leading slash\n",
    "        links.append(href)\n",
    "    return links\n",
    "\n",
    "def ensure_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "def download_file(url, dest_path, retries=3):\n",
    "    \"\"\"Download a file with retry logic.\"\"\"\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            resp = requests.get(url, headers=HEADERS, stream=True, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "            with open(dest_path, \"wb\") as f:\n",
    "                for chunk in resp.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error downloading {url} (attempt {attempt+1}/{retries}): {e}\")\n",
    "            time.sleep(2 * (attempt + 1))\n",
    "    return False\n",
    "\n",
    "def crawl_directory(rel_path=\"\", local_dir=\"data\"):\n",
    "    \"\"\"\n",
    "    Recursively crawl and download from the given relative path.\n",
    "    rel_path: path relative to BASE (\"\" means root)\n",
    "    local_dir: local directory to mirror\n",
    "    \"\"\"\n",
    "    full_url = urljoin(BASE, rel_path)\n",
    "    links = list_dir(full_url)\n",
    "    ensure_dir(local_dir)\n",
    "\n",
    "    for href in links:\n",
    "        href = href.lstrip(\"/\")  # normalize\n",
    "\n",
    "        # üö´ Skip links that \"jump back up\" (avoid a2d/31/a2d/ loops)\n",
    "        if rel_path and not href.startswith(rel_path) and \"/\" in href:\n",
    "            continue\n",
    "\n",
    "        # If href repeats rel_path (like 'a2d/31/'), reduce to just last part\n",
    "        if rel_path and href.startswith(rel_path):\n",
    "            href = os.path.basename(href.rstrip(\"/\")) + (\"/\" if href.endswith(\"/\") else \"\")\n",
    "\n",
    "        if href.endswith(\"/\"):  # it's a directory\n",
    "            sub_rel = posixpath.join(rel_path, href)\n",
    "            sub_local = os.path.join(local_dir, href.rstrip(\"/\"))\n",
    "            crawl_directory(sub_rel, sub_local)\n",
    "        else:  # it's a file\n",
    "            if ONLY_JSON and not href.lower().endswith(\".json\"):\n",
    "                continue  # skip non-JSON files if filter is enabled\n",
    "\n",
    "            remote_file = posixpath.join(rel_path, href)\n",
    "            local_file = os.path.join(local_dir, href)\n",
    "            if os.path.exists(local_file):\n",
    "                continue  # skip already downloaded\n",
    "            print(\"‚¨áÔ∏è Downloading\", remote_file)\n",
    "            download_file(urljoin(BASE, remote_file), local_file)\n",
    "            time.sleep(0.2)  # politeness delay\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # First download metadata\n",
    "    meta_files = [\"ReportersMetadata.json\", \"VolumesMetadata.json\", \"JurisdictionsMetadata.json\"]\n",
    "    ensure_dir(\"data\")\n",
    "    for mf in meta_files:\n",
    "        print(\"‚¨áÔ∏è Downloading metadata:\", mf)\n",
    "        download_file(BASE + mf, os.path.join(\"data\", mf))\n",
    "\n",
    "    # Then crawl everything else\n",
    "    crawl_directory(\"\", \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
