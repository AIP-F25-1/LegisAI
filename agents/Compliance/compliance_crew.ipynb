{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e21245e4",
   "metadata": {},
   "source": [
    "# Compliance and Risk "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a25b05",
   "metadata": {},
   "source": [
    "This notebook is **fully step-by-step**. It supports two input modes:\n",
    "1) **Chat Mode:** enter a prompt → agent responds → confidence estimated → if < 60%, auto redirect to human.  \n",
    "2) **Document Mode:** provide a contract file path (PDF/DOCX/TXT) → extract → summarize → risk scan → Monte Carlo → chart + saved artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfa04e8",
   "metadata": {},
   "source": [
    "### 1. Load environment + print active config (Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd3d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "ENV_PATH = \".env\"\n",
    "AGENTS_PATH = \"agents.yaml\"\n",
    "TASKS_PATH  = \"tasks.yaml\"\n",
    "\n",
    "print(\"Has .env? \", ENV_PATH.exists())\n",
    "print(\"Has agents.yaml? \", AGENTS_PATH.exists())\n",
    "print(\"Has tasks.yaml? \", TASKS_PATH.exists())\n",
    "\n",
    "# Load .env values\n",
    "load_dotenv(dotenv_path=ENV_PATH)\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\", \"ollama\")\n",
    "OPENAI_BASE_URL = os.getenv(\"OPENAI_BASE_URL\", \"http://localhost:11434/v1\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"llama3.1:8b-instruct\")\n",
    "OPENAI_TEMPERATURE = float(os.getenv(\"OPENAI_TEMPERATURE\", \"0.2\"))\n",
    "OPENAI_MAX_TOKENS = int(os.getenv(\"OPENAI_MAX_TOKENS\", \"2048\"))\n",
    "\n",
    "EMBED_MODEL = os.getenv(\"EMBED_MODEL\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "CHROMA_DIR = os.getenv(\"CHROMA_DIR\", \"./vectorstore\")\n",
    "\n",
    "print(\"Active LLM:\", OPENAI_MODEL)\n",
    "print(\"Base URL  :\", OPENAI_BASE_URL)\n",
    "print(\"Temperature:\", OPENAI_TEMPERATURE)\n",
    "print(\"Max tokens :\", OPENAI_MAX_TOKENS)\n",
    "print(\"Embeddings :\", EMBED_MODEL)\n",
    "print(\"Chroma dir :\", CHROMA_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c4b6e9",
   "metadata": {},
   "source": [
    "### 2. Load Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714600f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open(AGENTS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    agents_cfg = yaml.safe_load(f)\n",
    "with open(TASKS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    tasks_cfg = yaml.safe_load(f)\n",
    "\n",
    "print(\"Agents:\", list(agents_cfg.keys()))\n",
    "print(\"Tasks :\", list(tasks_cfg.keys()))\n",
    "\n",
    "print(\"\\n--- Example agent (compliance_checker) ---\")\n",
    "print(json.dumps(agents_cfg.get(\"compliance_checker\", {}), indent=2))\n",
    "\n",
    "print(\"\\n--- Example task (risk_task) ---\")\n",
    "print(json.dumps(tasks_cfg.get(\"risk_task\", {}), indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de88aed",
   "metadata": {},
   "source": [
    "### 3. Initialize the LLM (LangChain + OpenAI-compatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a2b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses Ollama via OpenAI-compatible REST (base_url in .env)\n",
    "try:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "except Exception:\n",
    "    from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    base_url=OPENAI_BASE_URL,\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model=OPENAI_MODEL,\n",
    "    temperature=OPENAI_TEMPERATURE,\n",
    "    max_tokens=OPENAI_MAX_TOKENS,\n",
    ")\n",
    "\n",
    "def llm_complete(system_prompt: str, user_prompt: str) -> str:\n",
    "    \"\"\"One-shot chat completion using system + user messages.\"\"\"\n",
    "    try:\n",
    "        resp = llm.invoke([\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ])\n",
    "        return getattr(resp, \"content\", str(resp))\n",
    "    except Exception as e:\n",
    "        return f\"[LLM error: {e}]\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9228ae",
   "metadata": {},
   "source": [
    "### 4. Utilities: text extraction, confidence scoring, risk model, Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5175df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, random\n",
    "from PyPDF2 import PdfReader\n",
    "import docx2txt\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def estimate_confidence(text: str) -> float:\n",
    "    \"\"\"Heuristic confidence 0..1 based on hedging vs certainty keywords.\"\"\"\n",
    "    if not text: return 0.0\n",
    "    lowered = text.lower()\n",
    "    hedges = [\"might\",\"may\",\"possibly\",\"unclear\",\"not sure\",\"cannot determine\",\"ambiguous\"]\n",
    "    certs  = [\"must\",\"shall\",\"clearly\",\"definitely\",\"is required\",\"complies\",\"non-compliant\"]\n",
    "    score = 0.5\n",
    "    for h in hedges:\n",
    "        if h in lowered: score -= 0.05\n",
    "    for c in certs:\n",
    "        if c in lowered: score += 0.05\n",
    "    return max(0.0, min(1.0, score))\n",
    "\n",
    "def extract_text_from_file(path: str) -> str:\n",
    "    \"\"\"Support PDF, DOCX, TXT/MD for quick prototyping.\"\"\"\n",
    "    ext = Path(path).suffix.lower()\n",
    "    if ext == \".pdf\":\n",
    "        try:\n",
    "            reader = PdfReader(path)\n",
    "            return \"\\n\".join([p.extract_text() or \"\" for p in reader.pages])\n",
    "        except Exception as e:\n",
    "            return f\"[PDF parse error: {e}]\"\n",
    "    if ext == \".docx\":\n",
    "        try:\n",
    "            return docx2txt.process(path) or \"\"\n",
    "        except Exception as e:\n",
    "            return f\"[DOCX parse error: {e}]\"\n",
    "    if ext in {\".txt\",\".md\"}:\n",
    "        try:\n",
    "            return Path(path).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        except Exception as e:\n",
    "            return f\"[TXT read error: {e}]\"\n",
    "    return \"[Unsupported file type. Use PDF, DOCX, or TXT.]\"\n",
    "\n",
    "# Lightweight keyword-based risk model (extend with your domain packs)\n",
    "RISK_KEYWORDS = {\n",
    "    \"liability\": 2.0, \"indemnify\": 2.5, \"indemnification\": 2.5, \"penalty\": 1.5,\n",
    "    \"termination\": 1.8, \"default\": 2.2, \"breach\": 2.0, \"confidentiality\": 1.2,\n",
    "    \"data protection\": 2.0, \"gdpr\": 2.0, \"ccpa\": 2.0, \"hipaa\": 2.0,\n",
    "    \"warranty\": 1.2, \"limitation of liability\": 2.3, \"force majeure\": 1.0,\n",
    "    \"governing law\": 0.8, \"arbitration\": 1.0\n",
    "}\n",
    "\n",
    "def keyword_risk_scan(text: str):\n",
    "    \"\"\"Return (hits, total_weighted, normalized_risk_0to1).\"\"\"\n",
    "    lowered = text.lower()\n",
    "    hits, total = {}, 0.0\n",
    "    for k,w in RISK_KEYWORDS.items():\n",
    "        c = lowered.count(k)\n",
    "        if c>0:\n",
    "            hits[k] = {\"count\":c,\"weight\":w,\"weighted\":c*w}\n",
    "            total += c*w\n",
    "    normalized = 1 - math.exp(-0.1*total)\n",
    "    return hits, total, normalized\n",
    "\n",
    "def monte_carlo_risk(base_risk: float, n: int = 2000):\n",
    "    \"\"\"Simulate incidents with mean probability ~= base_risk (Gaussian jitter).\"\"\"\n",
    "    rng = random.Random(42)\n",
    "    incidents = []\n",
    "    for _ in range(n):\n",
    "        p = max(0.0, min(1.0, rng.gauss(mu=base_risk, sigma=0.1)))\n",
    "        incidents.append(1 if rng.random() < p else 0)\n",
    "    incident_rate = sum(incidents)/n\n",
    "    return 1 - incident_rate, incident_rate, incidents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b18c6b",
   "metadata": {},
   "source": [
    "### 5. Chat Mode: define system & function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2346199f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_SYSTEM = f\"\"\"\n",
    "You are {agents_cfg.get('compliance_checker', {}).get('role', 'a compliance agent')}.\n",
    "Goal: {agents_cfg.get('compliance_checker', {}).get('goal', 'Ensure clause compliance and point out risks.')}\n",
    "Be precise, reference frameworks at a high level (GDPR/CCPA/HIPAA/Labor), and propose practical amendments.\n",
    "\"\"\"\n",
    "\n",
    "def run_chat_mode(user_prompt: str, confidence_threshold: float = 0.60):\n",
    "    reply = llm_complete(CHAT_SYSTEM, user_prompt)\n",
    "    conf = estimate_confidence(reply)\n",
    "    routed = False\n",
    "    if conf < confidence_threshold:\n",
    "        reply = reply.strip() + f\"\\n\\n— Confidence {conf*100:.0f}% < {confidence_threshold*100:.0f}% threshold. Redirecting to human agent...\"\n",
    "        routed = True\n",
    "    return {\"reply\": reply, \"confidence\": conf, \"routed_to_human\": routed}\n",
    "\n",
    "# Change this prompt and run next cell to execute\n",
    "chat_prompt = \"Is this data processing clause compliant with GDPR cross-border transfer rules?\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380f0418",
   "metadata": {},
   "source": [
    "### 6. Run Chat Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0951f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_chat_mode(chat_prompt, confidence_threshold=0.60)\n",
    "print(\"Reply:\\n\", result[\"reply\"])\n",
    "print(\"\\nConfidence:\", f\"{result['confidence']*100:.1f}%\")\n",
    "print(\"Redirected to human?\", result[\"routed_to_human\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeedd131",
   "metadata": {},
   "source": [
    "### 7. Document Mode: function to analyze a contract file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754cc353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv\n",
    "\n",
    "DOC_SYSTEM = f\"\"\"\n",
    "You are a contract analysis specialist combining roles of:\n",
    "- {agents_cfg.get('compliance_checker', {}).get('role', 'Compliance Checker')}\n",
    "- {agents_cfg.get('risk_assessor', {}).get('role', 'Risk Assessor')}\n",
    "Produce a concise summary, list key obligations, flag potential non-compliance, and suggest concrete clause updates.\n",
    "\"\"\"\n",
    "\n",
    "OUTPUT_DIR = PROJECT_DIR / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def analyze_document(file_path: str):\n",
    "    # 1) Extract\n",
    "    text = extract_text_from_file(file_path)\n",
    "    if text.startswith(\"[\") and \"error\" in text.lower(): return {\"error\": text}\n",
    "    if text.startswith(\"[Unsupported\"): return {\"error\": text}\n",
    "\n",
    "    # 2) Summarize with LLM\n",
    "    user_prompt = (\n",
    "        \"Summarize the following contract text. Provide:\\n\"\n",
    "        \"1) 5-8 bullet key points\\n\"\n",
    "        \"2) Notable obligations and data handling\\n\"\n",
    "        \"3) Potential compliance gaps\\n\"\n",
    "        \"4) Suggested clause updates (short, actionable)\\n\\n\"\n",
    "        f\"---\\n{text[:12000]}\"\n",
    "    )\n",
    "    summary = llm_complete(DOC_SYSTEM, user_prompt)\n",
    "\n",
    "    # 3) Risk scan + 4) Monte Carlo simulation\n",
    "    hits, total_w, norm_risk = keyword_risk_scan(text)\n",
    "    success_rate, incident_rate, incidents = monte_carlo_risk(norm_risk, n=2000)\n",
    "\n",
    "    # 5) Clause-level proxy scores (split on blank lines)\n",
    "    paragraphs = [p.strip() for p in text.split(\"\\n\\n\") if p.strip()]\n",
    "    clause_scores = []\n",
    "    for i, p in enumerate(paragraphs[:50]):\n",
    "        _, tw, nr = keyword_risk_scan(p)\n",
    "        clause_scores.append({\"clause_id\": i+1, \"risk_weight\": tw, \"risk_score\": nr})\n",
    "\n",
    "    # 6) Save artifacts (JSON + CSV)\n",
    "    base = Path(file_path).stem\n",
    "    json_path = OUTPUT_DIR / f\"{base}_analysis.json\"\n",
    "    csv_path  = OUTPUT_DIR / f\"{base}_clause_risks.csv\"\n",
    "\n",
    "    out = {\n",
    "        \"file\": file_path,\n",
    "        \"summary\": summary,\n",
    "        \"risk_keyword_hits\": hits,\n",
    "        \"total_risk_weight\": total_w,\n",
    "        \"normalized_risk_0to1\": norm_risk,\n",
    "        \"monte_carlo\": {\n",
    "            \"success_rate\": success_rate,\n",
    "            \"incident_rate\": incident_rate,\n",
    "            \"trials\": len(incidents)\n",
    "        },\n",
    "        \"clause_scores\": clause_scores,\n",
    "    }\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(out, f, indent=2)\n",
    "    with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        w = csv.DictWriter(f, fieldnames=[\"clause_id\",\"risk_weight\",\"risk_score\"])\n",
    "        w.writeheader(); w.writerows(clause_scores)\n",
    "\n",
    "    # 7) Visualize simulation outcomes\n",
    "    plt.figure()\n",
    "    plt.hist(incidents, bins=2)\n",
    "    plt.title(\"Monte Carlo Incident Outcomes (0=no incident, 1=incident)\")\n",
    "    plt.xlabel(\"Outcome\"); plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Saved:\", json_path); print(\"Saved:\", csv_path)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a669afb",
   "metadata": {},
   "source": [
    "### 8. Run Document Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a667f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to your contract file (PDF/DOCX/TXT). For demo, we create a tiny TXT.\n",
    "example_doc_path = PROJECT_DIR / \"example_contract.txt\"\n",
    "if not example_doc_path.exists():\n",
    "    example_doc_path.write_text(\n",
    "        \"This Agreement includes a limitation of liability clause.\\n\"\n",
    "        \"Either party may terminate for breach upon written notice.\\n\"\n",
    "        \"Data Protection: Controller shall comply with GDPR and CCPA.\\n\"\n",
    "        \"Indemnification applies in case of third-party claims arising from default.\",\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "out = analyze_document(str(example_doc_path))\n",
    "if \"error\" in out:\n",
    "    print(out[\"error\"])\n",
    "else:\n",
    "    print(\"\\n=== Summary (LLM) ===\\n\", out[\"summary\"][:1500])\n",
    "    print(\"\\nTotal Risk Weight:\", out[\"total_risk_weight\"])\n",
    "    print(\"Normalized Risk (0..1):\", f\"{out['normalized_risk_0to1']:.3f}\")\n",
    "    print(\"Incident Rate (Monte Carlo):\", f\"{out['monte_carlo']['incident_rate']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe7687",
   "metadata": {},
   "source": [
    "### 9. Inspect loaded YAML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c304aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AGENTS\\n======\")\n",
    "print(json.dumps(agents_cfg, indent=2))\n",
    "print(\"\\nTASKS\\n=====\")\n",
    "print(json.dumps(tasks_cfg, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d908f4c1",
   "metadata": {},
   "source": [
    "### 10. RAG setup: embeddings + Chroma client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f06106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Where to persist the vector DB (uses CHROMA_DIR from .env, but scoped under project)\n",
    "CHROMA_PATH = Path(PROJECT_DIR) / \"vectorstore\"\n",
    "CHROMA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize Chroma client (persistent)\n",
    "chroma_client = chromadb.PersistentClient(path=str(CHROMA_PATH))\n",
    "\n",
    "# Embedding function (Sentence-Transformers)\n",
    "# Will download the model on first use; cached afterward.\n",
    "EMBED_MODEL_NAME = EMBED_MODEL  # from .env (e.g., all-MiniLM-L6-v2)\n",
    "st_ef = embedding_functions.SentenceTransformerEmbeddingFunction(model_name=EMBED_MODEL_NAME)\n",
    "\n",
    "# Create or get a collection\n",
    "COLLECTION_NAME = \"compliance_docs\"\n",
    "collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME, embedding_function=st_ef)\n",
    "\n",
    "print(\"Vector store path:\", CHROMA_PATH)\n",
    "print(\"Collection:\", COLLECTION_NAME)\n",
    "print(\"Embedding model:\", EMBED_MODEL_NAME)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
