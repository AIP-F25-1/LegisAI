{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "from rank_bm25 import BM25Okapi\n",
    "import pickle\n",
    "\n",
    "# ---------CONFIGURATION---------\n",
    "cases_folder = \"D:/AIP/data/\"  # <--- change this to your folder if needed\n",
    "\n",
    "# ---------LOAD & EMBED CASES---------\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "embeddings = []\n",
    "meta = []\n",
    "bm25_corpus = []\n",
    "\n",
    "def extract_text(case):\n",
    "    casebody = case.get(\"casebody\", {})\n",
    "    text_parts = []\n",
    "    # Extract opinion text(s)\n",
    "    opinions = casebody.get(\"opinions\", [])\n",
    "    for item in opinions:\n",
    "        if isinstance(item, dict):\n",
    "            content = item.get(\"text\")\n",
    "            if content and len(content) > 20:\n",
    "                text_parts.append(content)\n",
    "    # Optionally add head_matter, parties, etc.\n",
    "    head_matter = casebody.get(\"head_matter\")\n",
    "    if isinstance(head_matter, str) and len(head_matter) > 20:\n",
    "        text_parts.append(head_matter)\n",
    "    parties = casebody.get(\"parties\", [])\n",
    "    for party in parties:\n",
    "        if isinstance(party, str) and len(party) > 20:\n",
    "            text_parts.append(party)\n",
    "    # Concatenate all parts\n",
    "    return \"\\n\".join(text_parts)\n",
    "\n",
    "for filename in os.listdir(cases_folder):\n",
    "    if filename.endswith('.json'):\n",
    "        with open(os.path.join(cases_folder, filename), 'r', encoding='utf-8') as f:\n",
    "            case = json.load(f)\n",
    "        full_text = extract_text(case)\n",
    "        if len(full_text) > 50:\n",
    "            emb = model.encode(full_text)\n",
    "            embeddings.append(emb)\n",
    "            meta.append({'id': case.get(\"id\"), 'name': case.get(\"name\"), 'filename': filename, 'text': full_text})\n",
    "            bm25_corpus.append(full_text.lower().split())\n",
    "        print(f\"Processing {filename} | Text length: {len(full_text)}\")\n",
    "\n",
    "print(f\"Total cases processed: {len(embeddings)}\")\n",
    "if len(embeddings) == 0:\n",
    "    print(\"No valid cases found. Check your folder and extraction logic.\")\n",
    "    exit()\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "# ---------FAISS SEMANTIC INDEX---------\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)\n",
    "\n",
    "# ---------BM25 KEYWORD INDEX---------\n",
    "bm25 = BM25Okapi(bm25_corpus)\n",
    "\n",
    "# ---------SEARCH FUNCTIONS---------\n",
    "\n",
    "def semantic_search(query, top_k=3):\n",
    "    query_emb = model.encode(query).reshape(1, -1)\n",
    "    _, I = index.search(query_emb, top_k)\n",
    "    results = [meta[i] for i in I[0]]\n",
    "    return results\n",
    "\n",
    "def keyword_search(query, top_k=3):\n",
    "    tokenized_query = query.lower().split()\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    top_indices = np.argsort(scores)[-top_k:][::-1]\n",
    "    return [meta[i] for i in top_indices]\n",
    "\n",
    "def hybrid_search(query, top_k=3, weight_semantic=0.5, weight_keyword=0.5):\n",
    "    # Get results & scores\n",
    "    query_emb = model.encode(query).reshape(1, -1)\n",
    "    D, I = index.search(query_emb, len(meta))  # dists, indices\n",
    "    semantic_scores = -D[0]  # negative distance (higher is better)\n",
    "    tokenized_query = query.lower().split()\n",
    "    keyword_scores = bm25.get_scores(tokenized_query)\n",
    "    # Normalize\n",
    "    s_norm = (semantic_scores - semantic_scores.min()) / (np.ptp(semantic_scores) + 1e-9)\n",
    "    k_norm = (keyword_scores - np.min(keyword_scores)) / (np.ptp(keyword_scores) + 1e-9)\n",
    "    hybrid = weight_semantic * s_norm + weight_keyword * k_norm\n",
    "    top_indices = np.argsort(hybrid)[-top_k:][::-1]\n",
    "    return [meta[i] for i in top_indices]\n",
    "\n",
    "# ---------EXAMPLE USAGE---------\n",
    "search_query = \"insurance payout after accidental death\"\n",
    "print(\"\\n--- Semantic Search ---\")\n",
    "for r in semantic_search(search_query):\n",
    "    print(f\"Case ID: {r['id']}, Name: {r['name']}, File: {r['filename']}\")\n",
    "\n",
    "print(\"\\n--- Keyword (BM25) Search ---\")\n",
    "for r in keyword_search(search_query):\n",
    "    print(f\"Case ID: {r['id']}, Name: {r['name']}, File: {r['filename']}\")\n",
    "\n",
    "print(\"\\n--- Hybrid Search ---\")\n",
    "for r in hybrid_search(search_query):\n",
    "    print(f\"Case ID: {r['id']}, Name: {r['name']}, File: {r['filename']}\")"
   ],
   "id": "d3449d566797f21e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
